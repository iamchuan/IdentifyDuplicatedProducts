{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n# Identify Duplicated Products Using TF-IDF"],"metadata":{}},{"cell_type":"markdown","source":["Entity Resolution, or \"[Record linkage][wiki]\" is the term used by statisticians, epidemiologists, and historians, among others, to describe the process of joining records from one data source with another that describe the same entity. Our terms with the same meaning include, \"entity disambiguation/linking\", duplicate detection\", \"deduplication\", \"record matching\", \"(reference) reconciliation\", \"object identification\", \"data/information integration\", and \"conflation\".\n \nEntity Resolution (ER) refers to the task of finding records in a dataset that refer to the same entity across different data sources (e.g., data files, books, websites, databases). ER is necessary when joining datasets based on entities that may or may not share a common identifier (e.g., database key, URI, National identification number), as may be the case due to differences in record shape, storage location, and/or curator style or preference. A dataset that has undergone ER may be referred to as being cross-linked.\n\nEntity resolution is a common, yet difficult problem in data cleaning and integration. This lab will demonstrate how we can use Apache Spark to apply powerful and scalable text analysis techniques and perform entity resolution across two datasets of commercial products.\n\n[wiki]: https://en.wikipedia.org/wiki/Record_linkage"],"metadata":{}},{"cell_type":"markdown","source":["## Data\nData files for this assignment are from the [metric-learning](https://code.google.com/p/metric-learning/) project and can be found at:\n`cs100/lab3`\n \nThe directory contains the following files:\n* **Google.csv**, the Google Products dataset\n* **Amazon.csv**, the Amazon dataset\n* **Google_small.csv**, 200 records sampled from the Google data\n* **Amazon_small.csv**, 200 records sampled from the Amazon data\n* **Amazon_Google_perfectMapping.csv**, the \"gold standard\" mapping\n* **stopwords.txt**, a list of common English words\n \nBesides the data files, there is a \"gold standard\" file that contains all of the true mappings between entities in the two datasets. Every row in the gold standard file has a pair of record IDs (one Google, one Amazon) that belong to two record that describe the same thing in the real world. We will use the gold standard to evaluate our algorithms."],"metadata":{}},{"cell_type":"code","source":["dirPath = '/databricks-datasets/cs100/lab3/data-001/'\ndisplay(dbutils.fs.ls(dirPath))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Part 1: Preliminaries"],"metadata":{}},{"cell_type":"markdown","source":["### 1.1 Import data\nWe read in each of the files and create spark DataFrames using `spark.read.csv()`. We take the first line of each file as its header and use `inferSchema=True` to let it automatically set schemas."],"metadata":{}},{"cell_type":"code","source":["import os\n\nprint(dbutils.fs.head(os.path.join(dirPath, 'Amazon.csv'), 500))\nprint('\\n')\nprint(dbutils.fs.head(os.path.join(dirPath, 'Google.csv'), 500))\nprint('\\n')\nprint(dbutils.fs.head(os.path.join(dirPath, 'Amazon_Google_perfectMapping.csv'), 500))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["amazonDF = spark.read.csv(path=os.path.join(dirPath, 'Amazon.csv'),\n                          header=True, \n                          inferSchema=True, )\namazonDF.printSchema()\namazonDF.show(5)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["googleDF = spark.read.csv(path=os.path.join(dirPath, 'Google.csv'),\n                        header=True, \n                        inferSchema=True)\ngoogleDF.printSchema()\ngoogleDF.show(5)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["perfectMapping = spark.read.csv(path=os.path.join(dirPath, 'Amazon_Google_perfectMapping.csv'), \n                                header=True,\n                                inferSchema=True)\nperfectMapping.printSchema()\nperfectMapping.show(5)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### 1.2 Impute data\nHere are a few changes we need to make on the two dataframes:\n- Change the `price` column in **googleDF** from `string` type to `double`.\n- Change the `name` column in **googleDF** to `title` to make the two dataframes have consistent headers.\n- Fill all `null` (string) fields with `''` in both **amazonDF** and **googleDF** (`null` value will cause error in text analysis later)."],"metadata":{}},{"cell_type":"code","source":["amazonDF = amazonDF.na.fill({'price':0.0}).na.fill('')\n\ngoogleDF = googleDF\\\n  .withColumn('price', googleDF['price'].cast('double'))\\\n  .withColumnRenamed('name', 'title')\\\n  .na.fill({'price':0.0})\\\n  .na.fill('')"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["amazonDF.printSchema()\ngoogleDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### 1.3 Filter rows\n\nWe want to make sure all the `id`s in both `amazonDF` and `googleDF` can be found in `perfectMapping`. Therefore we will eliminate those that do not exist in `perfectMapping`."],"metadata":{}},{"cell_type":"code","source":["amazonId = perfectMapping.select('idAmazon').distinct().withColumnRenamed('idAmazon', 'id')\namazonDF = amazonDF.join(amazonId, 'id', 'inner')\nprint '# of items in amazonDF:', amazonDF.count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["googleId = perfectMapping.select('idGoogleBase').distinct().withColumnRenamed('idGoogleBase', 'id')\ngoogleDF = googleDF.join(googleId, 'id', 'inner')\nprint '# of items in googleDF:', googleDF.count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### 1.4 Repartition data\n\nThe default dataFrames all have numbers of partitons equal 1, whereas the vCPU has 8 cores. So we want to repartition the dataframe for a better performance."],"metadata":{}},{"cell_type":"code","source":["print 'amazonDF part #:', amazonDF.rdd.getNumPartitions()\nprint 'googleDF part #:', googleDF.rdd.getNumPartitions()\nprint 'perfectMapping part #:', perfectMapping.rdd.getNumPartitions()\nprint 'Default parallelism:', sc.defaultParallelism"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["amazonDF = spark.createDataFrame(amazonDF.rdd.repartition(4))\ngoogleDF = spark.createDataFrame(googleDF.rdd.repartition(4))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## Part 2: Bag-of-Words Model and TF-IDF\n\n\n**Bag-of-Words**\n\nA simple approach to entity resolution is to treat all records as strings and compute their similarity with a string distance function. In this part, we will build some components for performing bag-of-words text-analysis, and then use them to compute record similarity.\n*[Bag-of-words][bag-of-words]* is a conceptually simple yet powerful approach to text analysis.\n \nThe idea is to treat strings, a.k.a. **documents**, as *unordered collections* of words, or **tokens**, i.e., as bags of words.\n> **Note on terminology**: a \"token\" is the result of parsing the document down to the elements we consider \"atomic\" for the task at hand.  Tokens can be things like words, numbers, acronyms, or other exotica like word-roots or fixed-length character strings.\n\n> Bag of words techniques all apply to any sort of token, so when we say \"bag-of-words\" we really mean \"bag-of-tokens,\" strictly speaking.\nTokens become the atomic unit of text comparison. If we want to compare two documents, we count how many tokens they share in common. If we want to search for documents with keyword queries (this is what Google does), then we turn the keywords into tokens and find documents that contain them. The power of this approach is that it makes string comparisons insensitive to small differences that probably do not affect meaning much, for example, punctuation and word order.\n\n**Tokenize a String**: *[Tokenization][Tokenization]* is the process of taking text (such as a sentence) and breaking it into individual terms (usually words). \n\n`RegexTokenizer` allows more advanced tokenization based on regular expression (regex) matching. By default, the parameter “pattern” (regex, default: `\"\\\\s+\"`) is used as delimiters to split the input text. Alternatively, users can set parameter “gaps” to false indicating the regex “pattern” denotes “tokens” rather than splitting gaps, and find all matching occurrences as the tokenization result.\n\n**Remove stopwords**: *[Stopwords][stopwords]* are common (English) words that do not contribute much to the content or meaning of a document (e.g., \"the\", \"a\", \"is\", \"to\", etc.). Stopwords add noise to bag-of-words comparisons, so they are usually excluded.\n\n`StopWordsRemover` takes as input a sequence of strings (e.g. the output of a Tokenizer) and drops all the stop words from the input sequences. The list of stopwords is specified by the `stopWords` parameter.\n\n**TF-IDF**\n\nBag-of-words comparisons are not very good when all tokens are treated the same: some tokens are more important than others. Weights give us a way to specify which tokens to favor. With weights, when we compare documents, instead of counting common tokens, we sum up the weights of common tokens. A good heuristic for assigning weights is called \"Term-Frequency/Inverse-Document-Frequency,\" or TF-IDF for short.\n\nTerm frequency-inverse document frequency ([TF-IDF][tfidf]) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Denote a term by *t*, a document by *d*, and the corpus by *D*. Term frequency *TF(t,d)* is the number of times that term *t* appears in document *d*, while document frequency *DF(t,D)* is the number of documents that contains term *t*. If we only use term frequency to measure the importance, it is very easy to over-emphasize terms that appear very often but carry little information about the document, e.g. “a”, “the”, and “of”. If a term appears very often across the corpus, it means it doesn’t carry special information about a particular document. Inverse document frequency is a numerical measure of how much information a term provides:\n\n$$IDF(t, D) = \\log\\frac{\\left|D\\right| + 1}{DF(t, D) + 1}.$$\n\nwhere *|D|* is the total number of documents in the corpus. Since logarithm is used, if a term appears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid dividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of TF and IDF:\n\n$$TFIDF(t,d,D)=TF(t,d)\\cdot IDF(t,D).$$\n\n\nThere are several variants on the definition of term frequency and document frequency. In MLlib, we separate TF and IDF to make them flexible.\n\n**TF**: Both `HashingTF` and `CountVectorizer` can be used to generate the term frequency vectors.\n\n`HashingTF` is a *Transformer* which takes sets of terms and converts those sets into fixed-length feature vectors. In text processing, a \"set of terms\" might be a bag of words. HashingTF utilizes the hashing trick. A raw feature is mapped into an index (term) by applying a hash function. The hash function used here is MurmurHash 3. Then term frequencies are calculated based on the mapped indices. This approach avoids the need to compute a global term-to-index map, which can be expensive for a large corpus, but it suffers from potential hash collisions, where different raw features may become the same term after hashing. To reduce the chance of collision, we can increase the target feature dimension, i.e. the number of buckets of the hash table. Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the feature dimension, otherwise the features will not be mapped evenly to the columns. \n\n`CountVectorizer` converts text documents to vectors of term counts.\n\n**IDF**: IDF is an *Estimator* which is fit on a dataset and produces an *IDFModel*. The *IDFModel* takes feature vectors (generally created from `HashingTF` or `CountVectorizer`) and scales each column. Intuitively, it down-weights columns which appear frequently in a corpus.\n\n[tfidf]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n\n[stopwords]: https://en.wikipedia.org/wiki/Stop_words\n\n[Tokenization]: (https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)\n\n[bag-of-words]: https://en.wikipedia.org/wiki/Bag-of-words_model"],"metadata":{}},{"cell_type":"markdown","source":["### 2.1 Build a bag-of-words-TFIDF pipeline\n\nNow we want to create a pipeline to include all the above stages and then transform the `title`, `description`, and `manufacturer` columns in the combined dataset (`amazonDF` + `googleDF`).\n\nStages:\n\n- RegexTokenizer\n- StopWordsRemover\n- CountVectorizer\n- IDF\n\nAfter the transformation, we will only keep `id`, `price`, `titleIDF`, `descriptionIDF`, and `manufacturerIDF` for the next step."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n\n## columns for the stages\ncolumns = ['title', 'description', 'manufacturer'] \n## minDFs for CountVectorizer\nminDFs = {'title':2.0, 'description':4.0, 'manufacturer':2.0}\n\npreProcStages = []\n\nfor col in columns:\n  regexTokenizer = RegexTokenizer(gaps=False, pattern='\\w+', inputCol=col, outputCol=col+'Token')\n  stopWordsRemover = StopWordsRemover(inputCol=col+'Token', outputCol=col+'SWRemoved')\n  countVectorizer = CountVectorizer(minDF=minDFs[col], inputCol=col+'SWRemoved', outputCol=col+'TF')\n  idf = IDF(inputCol=col+'TF', outputCol=col+'IDF') \n  preProcStages += [regexTokenizer, stopWordsRemover, countVectorizer, idf]\n  \nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages=preProcStages)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["## combine two dataframes\ndataCombined = amazonDF.union(googleDF)\n## pipeline fit/transform\nmodel = pipeline.fit(dataCombined)\ndataCombined = model.transform(dataCombined)\n## select columns\ndataCombined = dataCombined.select('id', 'price', 'titleIDF', 'descriptionIDF', 'manufacturerIDF')"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["dataCombined.sample(False, .01).show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Part 3: Text Similarity and Price Similarity"],"metadata":{}},{"cell_type":"markdown","source":["### 3.1 Text Similarity\n\nNow we are ready to do text comparisons in a formal way. The metric of string distance we will use is called *[cosine similarity][cosine]*. We will treat each document as a vector in some high dimensional space. Then, to compare two documents we compute the cosine of the angle between their two document vectors.\n \nGiven two vectors *a* and *b*, the cosine of the angle between them can be calculated via the formula below:\n\n\\\\[ similarity = \\cos \\theta = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{\\sum a_i b_i}{\\sqrt{\\sum a_i^2} \\sqrt{\\sum b_i^2}} \\\\]\n \nSetting aside the algebra, the geometric interpretation is more intuitive. The angle between two document vectors is small if they share many tokens in common, because they are pointing in roughly the same direction. For that case, the cosine of the angle will be large. Otherwise, if the angle is large (and they have few words in common), the cosine is small. Therefore, cosine similarity scales proportionally with our intuitive sense of similarity.\n\nIn Spark, the `SparseVector` class provides methods `SparseVector.dot(other)` and `SparseVector.norm(p)`, which can be used for calculating the cosine similarity between two SparseVectors. [[source](http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector)]\n[cosine]: https://en.wikipedia.org/wiki/Cosine_similarity"],"metadata":{}},{"cell_type":"code","source":["import math\n\ndef cosine_similarity(X, Y):\n  denom = X.norm(2) * Y.norm(2)\n  if denom == 0.0:\n    return -1.0\n  else:\n    return X.dot(Y) / float(denom)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### 3.2 Price Similarity\n\nFor the prices of any two products, we define their price similarity via the formula below:\n\n\\\\[ price\\\\_similarity = 1 - \\frac{\\| X - Y \\|}{ X + Y } \\\\]"],"metadata":{}},{"cell_type":"code","source":["def price_similarity(X, Y):\n  denom = X + Y\n  if denom == 0.0:\n    return -1.0\n  else:\n    return 1 - abs(X - Y) / float(denom)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Let's test these functions with two matching products:"],"metadata":{}},{"cell_type":"code","source":["aID, gID = u'b000g80lqo', u'http://www.google.com/base/feeds/snippets/18441188461196475272'\n\naProd = dataCombined.filter(dataCombined['id'] == aID).collect()[0]\ngProd = dataCombined.filter(dataCombined['id'] == gID).collect()[0]\n\nprint amazonDF.filter(amazonDF['id'] == aID)\\\n    .select('title', 'price')\\\n    .union(googleDF.filter(googleDF['id'] == gID).select('title', 'price'))\\\n    .show(truncate=False)\n\nprint 'Title similarity: ', cosine_similarity(aProd['titleIDF'], gProd['titleIDF'])\nprint 'DES similarity:   ', cosine_similarity(aProd['descriptionIDF'], gProd['descriptionIDF'])\nprint 'MFR similarity:   ', cosine_similarity(aProd['manufacturerIDF'], gProd['manufacturerIDF'])\nprint 'Price similarity: ', price_similarity(aProd['price'], gProd['price'])"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### 3.3 Calculate similarity scores\n\nNow let's create a new dataframe which contains all the paired products from **amazon** and **google**, and then add similarity scores to each pair.\n\nTo do that, our plan is:\n\n1. Broadcast `dataCombined` as a lookup table.\n2. Define a function `similarities` to calculate all the four similarity scores for given `idAmazon` and `idGoogleBase`.\n3. Calculate the similarity scores for all pairs of `idAmazon` and `idGoogleBase` and then convert them to a dataframe.\n4. Create `label` column by performing `LEFT JOIN` with `perfectMapping` dataframe and impute `null` with 0."],"metadata":{}},{"cell_type":"code","source":["# Step 1\n# Broadcast dataCombined as a lookup table.\n# use collectAsMap() to create a python dict object for fast lookup\nlookupTable = sc.broadcast(dataCombined.rdd.map(lambda x: (x['id'], \n                                                           {'price':x['price'], \n                                                            'titleIDF':x['titleIDF'], \n                                                            'descriptionIDF':x['descriptionIDF'],\n                                                            'manufacturerIDF':x['manufacturerIDF']})).collectAsMap())"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# to get the value of an item, for example, u'b00002s8if', we can do:\nprint(lookupTable.value[u'b00002s8if']['price'])"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Step 2\n# Define a function similarities to calculate all the similarity scores for the given pair of idAmazon and idGoogleBase.\ndef similarities(idAmazon, idGoogleBase, lookupTable):\n  X, Y = lookupTable.value[idAmazon], lookupTable.value[idGoogleBase]\n  price_simi = price_similarity(X['price'], Y['price'])\n  title_simi = cosine_similarity(X['titleIDF'], Y['titleIDF'])\n  descr_simi = cosine_similarity(X['descriptionIDF'], Y['descriptionIDF'])\n  manuf_simi = cosine_similarity(X['manufacturerIDF'], Y['manufacturerIDF'])\n  return price_simi, title_simi, descr_simi, manuf_simi"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Step 3\n# Calculate the similarity scores for all pairs of idAmazon and idGoogleBase and then convert them to a dataframe.\nfrom pyspark.sql import Row\n\npairId = amazonDF.select('id').rdd.flatMap(list).cartesian(googleDF.select('id').rdd.flatMap(list))\npairProdDF = pairId.map(lambda x: x + similarities(x[0], x[1], lookupTable))\n\nmeasureMapping = spark.createDataFrame(pairProdDF.map(lambda x: Row(idAmazon=x[0], \n                                                                    idGoogleBase=x[1], \n                                                                    price_simi=float(x[2]),\n                                                                    title_simi=float(x[3]),\n                                                                    descr_simi=float(x[4]), \n                                                                    manuf_simi=float(x[5]))))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# Step 4\n# Create label column by performing LEFT JOIN with perfectMapping dataframe and impute null with 0.\n# For the JOIN operation, broadcast perfectMapping for efficiency\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import broadcast\n\nmeasureMapping = measureMapping.join(broadcast(perfectMapping.withColumn('label', lit(1.0))), \n                                     ['idAmazon', 'idGoogleBase'], \n                                     'left')\nmeasureMapping = measureMapping.na.fill({'label':0.0})"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["measureMapping.sample(True, .001).show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### 3.4 Cache data\n\nWe now can cache the dataframe for the next step - classifier building!"],"metadata":{}},{"cell_type":"markdown","source":["## Part 4: Building a Classifier"],"metadata":{}},{"cell_type":"markdown","source":["### 4.1 Training / Test Splitting"],"metadata":{}},{"cell_type":"markdown","source":["Now we can build our machine learning classifier to identify the true duplicate pairs. Let's first create our training / test datasets."],"metadata":{}},{"cell_type":"code","source":["measureMapping.cache()\nmeasureMapping.groupBy('label').count().show()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["train_0, test_0 = measureMapping.filter('label = 0.0').randomSplit([0.8, 0.2])\ntrain_1, test_1 = measureMapping.filter('label = 1.0').randomSplit([0.8, 0.2])\ntest = test_0.union(test_1)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["To visualize the distributions of all the similarity scores, we need to undersample the majority class (label 0) since the numbers are way more than (X1100) that of the minority class (label 1)."],"metadata":{}},{"cell_type":"code","source":["# trainUnderSampled contains 1% of train_0 and all train_1\ntrainUnderSampled = train_0.sample(False, .01).union(train_1)\ntrainUnderSampled = spark.createDataFrame(trainUnderSampled.rdd.repartition(8))\ntrainUnderSampled.groupBy('label').count().show()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["display(trainUnderSampled.sample(False, .07))"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### 4.2 DecisionTree Classifier\n\nLet's build a DecisionTree Classifier with the under sampled data first."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nassembler = VectorAssembler(inputCols=trainUnderSampled.columns[2:-1],\n                            outputCol='features')\ndt = DecisionTreeClassifier(labelCol='label', featuresCol='features')\npipeline = Pipeline(stages=[assembler, dt])\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(dt.maxDepth, [3, 5, 7]) \\\n    .build()\n\nevaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', \n                                          metricName='areaUnderROC')\n\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=5)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["cvModel = crossval.fit(trainUnderSampled)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["prediction = cvModel.transform(test)\ndisplay(prediction.groupBy('label', 'prediction').count())"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### 4.3 Model Evaluation\n\nLet's evaluate the decisionTree model with the test datset using a confusion matrix and a roc plot"],"metadata":{}},{"cell_type":"code","source":["AUC_score = evaluator.evaluate(prediction)\nprint 'AUC score: {}'.format(AUC_score)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["def roc_plot(prediction, labelCol, scoreCol, pos_label):\n  import matplotlib.pyplot as plt\n  from sklearn import metrics\n  \n  scores = prediction.select(scoreCol).rdd.map(lambda x: list(x)[0].toArray()[1]).collect()\n  labels = prediction.select(labelCol).rdd.flatMap(list).collect()\n  \n  fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=pos_label)\n#   auc_score = metrics.auc(fpr, tpr)\n  auc_score = evaluator.evaluate(prediction)\n  fig = plt.figure()\n  plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k')\n  plt.plot(fpr, tpr, color='g', linestyle='--',\n           label='Mean ROC (area = %0.3f)' % auc_score, lw=4)\n  plt.legend(loc=\"lower right\")\n  display(fig)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["roc_plot(prediction, labelCol='label', scoreCol='probability', pos_label=1.0)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["prediction.select('probability').show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["### 4.5 Ensemble of DecisionTree Classifiers"],"metadata":{}},{"cell_type":"markdown","source":["In order to use most of the data, we will now build an ensemble model or the DecisionTree classifiers."],"metadata":{}},{"cell_type":"code","source":["train_0 = spark.createDataFrame(train_0.rdd.repartition(4)).cache()\ntrain_1 = spark.createDataFrame(train_1.rdd.repartition(4)).cache()\ntrain_0_splits = train_0.rdd.randomSplit([1]*20)\n# measureMapping.unpersist()"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["train_1.union(spark.createDataFrame(train_0_splits[0].sample(False, .2))).rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["bestModels = []\nprint('Begin training')\nfor i, train_0_split in enumerate(train_0_splits):\n  print('Split: {}'.format(i))\n  underSampled = train_1.union(spark.createDataFrame(train_0_split.sample(False, .2)))\n  cvModel = crossval.fit(underSampled)\n  bestModels.append(cvModel.bestModel)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["for i, bestModel in enumerate(bestModels):\n  print('saving bestModel {}'.format(i))\n  bestModel.write().overwrite().save('taxiAnalysisER_bestModels/bm_{}'.format(i))"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\n\nbestModels = []\n\nfor i in xrange(20):\n  bestModels.append(PipelineModel.load('taxiAnalysisER_bestModels/bm_{}'.format(i)))"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\n\ndef voting(*votes):\n  return float(sum(votes) / len(votes) > .5)\n\nudfVoting=udf(voting, FloatType())\n\ndef majority_vote(models, data):\n  predictions = data.select('idAmazon', 'idGoogleBase', 'label')\n  for i, model in enumerate(models):\n    temp = model.transform(data)\\\n        .select('idAmazon', 'idGoogleBase', 'prediction')\\\n         .withColumnRenamed('prediction', 'prediction_'+str(i))\n    predictions = predictions.join(temp, ['idAmazon', 'idGoogleBase'], 'left')\n  predictions = predictions.withColumn('prediction', udfVoting(*predictions.columns[3:]))\n  return predictions"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["predictions = majority_vote(bestModels, test)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["display(predictions.groupBy('label', 'prediction').count())"],"metadata":{},"outputs":[],"execution_count":66}],"metadata":{"name":"taxt_analysis_entity_resolution","notebookId":2690176961360097},"nbformat":4,"nbformat_minor":0}
